In June 2018, Ryan Smith and I partnered on this kanban origami project where at the "MixedRealityAirlift" in Redmond.

Features
* VR project using WindowsMR and SteamVR
* Grab origami figures that would unfold as you moved them towards your face
* Used Azure Cognitive Services: Voice and Vision to capture voice streams and transcribe and structure the voices into
  commands that would attatch itself to the 3d origami art.
* bind information into 3d representations, 
* multiple 3D spaces worlds where you could move the objects between.
  
  For example, you could reach your hands out, and grab a panda, as you moved the panda to your face the panda would unfold
  and you could speak text to the panda (e.g. remind frank about the TPS reports).  Azure would receive the voice stream
  in real time and then encode the instructions as a 'type of reminder', 'frank was the target' and the details 'were to remind frank about the 
  tps report'.  Now that structured data is embedded in that instance of a panda you could move or put he panda where you 
  wanted by moving (or throwing) the panda out through windows into separate 3d worlds or spaces.  e.g. one world might be red which might represent and urgent world
  and you might have a green relaxing world.  you could choose to put this panda in whatever world you wanted by reading in and out of windows that, from one
  center area, acted as a gateway to those worlds.
  
  
